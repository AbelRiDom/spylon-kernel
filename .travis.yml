sudo: false
dist: trusty
language: python

python:
  - "3.5"

before_install:
  # Install a version of Spark to test this against
  - wget http://d3kbcqa49mib13.cloudfront.net/spark-2.0.2-bin-hadoop2.7.tgz
  - tar xf spark-2.0.2-bin-hadoop2.7.tgz -C $HOME/.cache/spark

env:
  - SPARK_HOME=$HOME/.cache/spark/spark-2.0.2-bin-hadoop2.7 SPARK_SUBMIT_OPTS="-Dscala.usejavacp=true" PYTHONUNBUFFERED=1

install:
  - pip install -r requirements.txt
  - pip install .
  - pip install -r requirements-test.txt
  - python -m spylon_kernel install --user

script:
  - coverage run run_tests.py -vrsx --capture=sys --color=yes
  - coverage report -m
  # Ensure installability
  - python setup.py sdist
  - pip install --no-use-wheel dist/*.tar.gz

# Cache these at the end of the build
cache:
  directories:
    - $HOME/.cache/pip
    - $HOME/.cache/spark

after_success:
    - codecov
